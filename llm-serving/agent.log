/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/numpy/core/getlimits.py:499: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.
  setattr(self, word, getattr(machar, word).flat[0])
/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.
  return self._float_to_str(self.smallest_subnormal)
/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/numpy/core/getlimits.py:499: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.
  setattr(self, word, getattr(machar, word).flat[0])
/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.
  return self._float_to_str(self.smallest_subnormal)
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
load yaml sucess!
2024-11-01 00:10:54,770 - mindformers[mindformers/version_control.py:61] - INFO - The Cell Reuse compilation acceleration feature is not supported when the environment variable ENABLE_CELL_REUSE is 0 or MindSpore version is earlier than 2.1.0 or stand_alone mode or pipeline_stages <= 1
2024-11-01 00:10:54,770 - mindformers[mindformers/version_control.py:65] - INFO - 
The current ENABLE_CELL_REUSE=0, please set the environment variable as follows: 
export ENABLE_CELL_REUSE=1 to enable the Cell Reuse compilation acceleration feature.
2024-11-01 00:10:54,770 - mindformers[mindformers/version_control.py:71] - INFO - The Cell Reuse compilation acceleration feature does not support single-card mode.This feature is disabled by default. ENABLE_CELL_REUSE=1 does not take effect.
2024-11-01 00:10:54,770 - mindformers[mindformers/version_control.py:74] - INFO - The Cell Reuse compilation acceleration feature only works in pipeline parallel mode(pipeline_stage>1).Current pipeline stage=1, the feature is disabled by default.
[WARNING] ME(947189:281473796161712,Process-1):2024-11-01-00:11:16.633.298 [mindspore/ops/primitive.py:203] The in_strategy/in_layout of the operator in your network will not take effect in stand_alone mode. This means the the shard function called in the network is ignored. 
If you want to enable it, please use semi auto or auto parallel mode by context.set_auto_parallel_context(parallel_mode=ParallelMode.SEMI_AUTO_PARALLEL or context.set_auto_parallel_context(parallel_mode=ParallelMode.AUTO_PARALLEL)
2024-11-01 00:11:21,497 - mindformers[mindformers/models/llama/llama_transformer.py:468] - INFO - MoE config is None, use normal FFN
[WARNING] ME(947189:281473796161712,Process-1):2024-11-01-00:11:21.499.144 [mindspore/common/_decorator.py:40] 'Parameter' is deprecated from version 2.3 and will be removed in a future version, use 'add_pipeline_stage' instead.
[WARNING] ME(947189:281473796161712,Process-1):2024-11-01-00:11:21.499.302 [mindspore/common/parameter.py:806] This interface may be deleted in the future.
2024-11-01 00:11:24,188 - mindformers[mindformers/models/llama/llama_transformer.py:468] - INFO - MoE config is None, use normal FFN
2024-11-01 00:11:26,901 - mindformers[mindformers/models/llama/llama_transformer.py:468] - INFO - MoE config is None, use normal FFN
2024-11-01 00:11:29,599 - mindformers[mindformers/models/llama/llama_transformer.py:468] - INFO - MoE config is None, use normal FFN
2024-11-01 00:11:32,295 - mindformers[mindformers/models/llama/llama_transformer.py:468] - INFO - MoE config is None, use normal FFN
2024-11-01 00:11:34,995 - mindformers[mindformers/models/llama/llama_transformer.py:468] - INFO - MoE config is None, use normal FFN
2024-11-01 00:11:37,692 - mindformers[mindformers/models/llama/llama_transformer.py:468] - INFO - MoE config is None, use normal FFN
2024-11-01 00:11:40,395 - mindformers[mindformers/models/llama/llama_transformer.py:468] - INFO - MoE config is None, use normal FFN
2024-11-01 00:11:43,100 - mindformers[mindformers/models/llama/llama_transformer.py:468] - INFO - MoE config is None, use normal FFN
2024-11-01 00:11:45,743 - mindformers[mindformers/models/llama/llama_transformer.py:468] - INFO - MoE config is None, use normal FFN
[WARNING] ME(947189:281473796161712,Process-1):2024-11-01-00:13:40.127.780 [mindspore/train/serialization.py:195] The type of model.layers.0.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(947189:281473796161712,Process-1):2024-11-01-00:13:40.129.103 [mindspore/train/serialization.py:195] The type of model.layers.0.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(947189:281473796161712,Process-1):2024-11-01-00:13:40.460.867 [mindspore/train/serialization.py:195] The type of model.layers.1.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(947189:281473796161712,Process-1):2024-11-01-00:13:40.461.872 [mindspore/train/serialization.py:195] The type of model.layers.1.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(947189:281473796161712,Process-1):2024-11-01-00:13:40.793.115 [mindspore/train/serialization.py:195] The type of model.layers.2.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(947189:281473796161712,Process-1):2024-11-01-00:13:40.794.099 [mindspore/train/serialization.py:195] The type of model.layers.2.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(947189:281473796161712,Process-1):2024-11-01-00:13:41.125.718 [mindspore/train/serialization.py:195] The type of model.layers.3.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(947189:281473796161712,Process-1):2024-11-01-00:13:41.126.706 [mindspore/train/serialization.py:195] The type of model.layers.3.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(947189:281473796161712,Process-1):2024-11-01-00:13:41.457.013 [mindspore/train/serialization.py:195] The type of model.layers.4.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(947189:281473796161712,Process-1):2024-11-01-00:13:41.457.995 [mindspore/train/serialization.py:195] The type of model.layers.4.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(947189:281473796161712,Process-1):2024-11-01-00:13:41.789.756 [mindspore/train/serialization.py:195] The type of model.layers.5.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(947189:281473796161712,Process-1):2024-11-01-00:13:41.790.780 [mindspore/train/serialization.py:195] The type of model.layers.5.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(947189:281473796161712,Process-1):2024-11-01-00:13:42.122.695 [mindspore/train/serialization.py:195] The type of model.layers.6.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(947189:281473796161712,Process-1):2024-11-01-00:13:42.123.696 [mindspore/train/serialization.py:195] The type of model.layers.6.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(947189:281473796161712,Process-1):2024-11-01-00:13:42.456.187 [mindspore/train/serialization.py:195] The type of model.layers.7.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(947189:281473796161712,Process-1):2024-11-01-00:13:42.457.476 [mindspore/train/serialization.py:195] The type of model.layers.7.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(947189:281473796161712,Process-1):2024-11-01-00:13:42.789.816 [mindspore/train/serialization.py:195] The type of model.layers.8.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(947189:281473796161712,Process-1):2024-11-01-00:13:42.790.811 [mindspore/train/serialization.py:195] The type of model.layers.8.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(947189:281473796161712,Process-1):2024-11-01-00:13:43.123.046 [mindspore/train/serialization.py:195] The type of model.layers.9.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(947189:281473796161712,Process-1):2024-11-01-00:13:43.124.051 [mindspore/train/serialization.py:195] The type of model.layers.9.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(947189:281473796161712,Process-1):2024-11-01-00:13:43.453.678 [mindspore/train/serialization.py:195] The type of model.layers.10.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(947189:281473796161712,Process-1):2024-11-01-00:13:43.454.555 [mindspore/train/serialization.py:195] The type of model.layers.10.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(947189:281473796161712,Process-1):2024-11-01-00:13:43.782.138 [mindspore/train/serialization.py:195] The type of model.layers.11.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(947189:281473796161712,Process-1):2024-11-01-00:13:43.783.123 [mindspore/train/serialization.py:195] The type of model.layers.11.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(947189:281473796161712,Process-1):2024-11-01-00:13:44.111.816 [mindspore/train/serialization.py:195] The type of model.layers.12.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(947189:281473796161712,Process-1):2024-11-01-00:13:44.112.793 [mindspore/train/serialization.py:195] The type of model.layers.12.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(947189:281473796161712,Process-1):2024-11-01-00:13:44.440.397 [mindspore/train/serialization.py:195] The type of model.layers.13.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(947189:281473796161712,Process-1):2024-11-01-00:13:44.441.390 [mindspore/train/serialization.py:195] The type of model.layers.13.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(947189:281473796161712,Process-1):2024-11-01-00:13:44.768.540 [mindspore/train/serialization.py:195] The type of model.layers.14.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(947189:281473796161712,Process-1):2024-11-01-00:13:44.769.536 [mindspore/train/serialization.py:195] The type of model.layers.14.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(947189:281473796161712,Process-1):2024-11-01-00:13:45.988.15 [mindspore/train/serialization.py:195] The type of model.layers.15.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(947189:281473796161712,Process-1):2024-11-01-00:13:45.998.07 [mindspore/train/serialization.py:195] The type of model.layers.15.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(947189:281473796161712,Process-1):2024-11-01-00:13:45.429.012 [mindspore/train/serialization.py:195] The type of model.layers.16.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(947189:281473796161712,Process-1):2024-11-01-00:13:45.429.976 [mindspore/train/serialization.py:195] The type of model.layers.16.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(947189:281473796161712,Process-1):2024-11-01-00:13:45.771.378 [mindspore/train/serialization.py:195] The type of model.layers.17.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(947189:281473796161712,Process-1):2024-11-01-00:13:45.772.445 [mindspore/train/serialization.py:195] The type of model.layers.17.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(947189:281473796161712,Process-1):2024-11-01-00:13:46.122.948 [mindspore/train/serialization.py:195] The type of model.layers.18.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(947189:281473796161712,Process-1):2024-11-01-00:13:46.124.002 [mindspore/train/serialization.py:195] The type of model.layers.18.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(947189:281473796161712,Process-1):2024-11-01-00:13:46.476.126 [mindspore/train/serialization.py:195] The type of model.layers.19.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(947189:281473796161712,Process-1):2024-11-01-00:13:46.477.204 [mindspore/train/serialization.py:195] The type of model.layers.19.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(947189:281473796161712,Process-1):2024-11-01-00:13:46.833.529 [mindspore/train/serialization.py:195] The type of model.layers.20.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(947189:281473796161712,Process-1):2024-11-01-00:13:46.834.589 [mindspore/train/serialization.py:195] The type of model.layers.20.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(947189:281473796161712,Process-1):2024-11-01-00:13:47.177.377 [mindspore/train/serialization.py:195] The type of model.layers.21.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(947189:281473796161712,Process-1):2024-11-01-00:13:47.178.364 [mindspore/train/serialization.py:195] The type of model.layers.21.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(947189:281473796161712,Process-1):2024-11-01-00:13:47.515.476 [mindspore/train/serialization.py:195] The type of model.layers.22.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(947189:281473796161712,Process-1):2024-11-01-00:13:47.516.454 [mindspore/train/serialization.py:195] The type of model.layers.22.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(947189:281473796161712,Process-1):2024-11-01-00:13:47.851.798 [mindspore/train/serialization.py:195] The type of model.layers.23.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(947189:281473796161712,Process-1):2024-11-01-00:13:47.852.795 [mindspore/train/serialization.py:195] The type of model.layers.23.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(947189:281473796161712,Process-1):2024-11-01-00:13:48.187.227 [mindspore/train/serialization.py:195] The type of model.layers.24.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(947189:281473796161712,Process-1):2024-11-01-00:13:48.188.200 [mindspore/train/serialization.py:195] The type of model.layers.24.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(947189:281473796161712,Process-1):2024-11-01-00:13:48.528.037 [mindspore/train/serialization.py:195] The type of model.layers.25.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(947189:281473796161712,Process-1):2024-11-01-00:13:48.529.102 [mindspore/train/serialization.py:195] The type of model.layers.25.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(947189:281473796161712,Process-1):2024-11-01-00:13:48.868.833 [mindspore/train/serialization.py:195] The type of model.layers.26.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(947189:281473796161712,Process-1):2024-11-01-00:13:48.869.936 [mindspore/train/serialization.py:195] The type of model.layers.26.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(947189:281473796161712,Process-1):2024-11-01-00:13:49.205.738 [mindspore/train/serialization.py:195] The type of model.layers.27.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(947189:281473796161712,Process-1):2024-11-01-00:13:49.206.754 [mindspore/train/serialization.py:195] The type of model.layers.27.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(947189:281473796161712,Process-1):2024-11-01-00:13:49.540.082 [mindspore/train/serialization.py:195] The type of model.layers.28.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(947189:281473796161712,Process-1):2024-11-01-00:13:49.541.168 [mindspore/train/serialization.py:195] The type of model.layers.28.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(947189:281473796161712,Process-1):2024-11-01-00:13:49.874.640 [mindspore/train/serialization.py:195] The type of model.layers.29.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(947189:281473796161712,Process-1):2024-11-01-00:13:49.875.693 [mindspore/train/serialization.py:195] The type of model.layers.29.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(947189:281473796161712,Process-1):2024-11-01-00:13:50.210.347 [mindspore/train/serialization.py:195] The type of model.layers.30.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(947189:281473796161712,Process-1):2024-11-01-00:13:50.211.329 [mindspore/train/serialization.py:195] The type of model.layers.30.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(947189:281473796161712,Process-1):2024-11-01-00:13:50.544.661 [mindspore/train/serialization.py:195] The type of model.layers.31.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(947189:281473796161712,Process-1):2024-11-01-00:13:50.545.645 [mindspore/train/serialization.py:195] The type of model.layers.31.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(947189:281473796161712,Process-1):2024-11-01-00:13:50.878.785 [mindspore/train/serialization.py:195] The type of model.norm_out.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(947189:281473796161712,Process-1):2024-11-01-00:13:51.928.58 [mindspore/train/serialization.py:1456] For 'load_param_into_net', 64 parameters in the 'net' are not loaded, because they are not in the 'parameter_dict', please check whether the network structure is consistent when training and loading checkpoint.
[WARNING] ME(947189:281473796161712,Process-1):2024-11-01-00:13:51.932.78 [mindspore/train/serialization.py:1460] ['model.layers.0.attention.infer_attention.paged_attention_mgr.key_cache', 'model.layers.0.attention.infer_attention.paged_attention_mgr.value_cache', 'model.layers.1.attention.infer_attention.paged_attention_mgr.key_cache', 'model.layers.1.attention.infer_attention.paged_attention_mgr.value_cache', 'model.layers.2.attention.infer_attention.paged_attention_mgr.key_cache', 'model.layers.2.attention.infer_attention.paged_attention_mgr.value_cache', 'model.layers.3.attention.infer_attention.paged_attention_mgr.key_cache', 'model.layers.3.attention.infer_attention.paged_attention_mgr.value_cache', 'model.layers.4.attention.infer_attention.paged_attention_mgr.key_cache', 'model.layers.4.attention.infer_attention.paged_attention_mgr.value_cache', 'model.layers.5.attention.infer_attention.paged_attention_mgr.key_cache', 'model.layers.5.attention.infer_attention.paged_attention_mgr.value_cache', 'model.layers.6.attention.infer_attention.paged_attention_mgr.key_cache', 'model.layers.6.attention.infer_attention.paged_attention_mgr.value_cache', 'model.layers.7.attention.infer_attention.paged_attention_mgr.key_cache', 'model.layers.7.attention.infer_attention.paged_attention_mgr.value_cache', 'model.layers.8.attention.infer_attention.paged_attention_mgr.key_cache', 'model.layers.8.attention.infer_attention.paged_attention_mgr.value_cache', 'model.layers.9.attention.infer_attention.paged_attention_mgr.key_cache', 'model.layers.9.attention.infer_attention.paged_attention_mgr.value_cache', 'model.layers.10.attention.infer_attention.paged_attention_mgr.key_cache', 'model.layers.10.attention.infer_attention.paged_attention_mgr.value_cache', 'model.layers.11.attention.infer_attention.paged_attention_mgr.key_cache', 'model.layers.11.attention.infer_attention.paged_attention_mgr.value_cache', 'model.layers.12.attention.infer_attention.paged_attention_mgr.key_cache', 'model.layers.12.attention.infer_attention.paged_attention_mgr.value_cache', 'model.layers.13.attention.infer_attention.paged_attention_mgr.key_cache', 'model.layers.13.attention.infer_attention.paged_attention_mgr.value_cache', 'model.layers.14.attention.infer_attention.paged_attention_mgr.key_cache', 'model.layers.14.attention.infer_attention.paged_attention_mgr.value_cache', 'model.layers.15.attention.infer_attention.paged_attention_mgr.key_cache', 'model.layers.15.attention.infer_attention.paged_attention_mgr.value_cache', 'model.layers.16.attention.infer_attention.paged_attention_mgr.key_cache', 'model.layers.16.attention.infer_attention.paged_attention_mgr.value_cache', 'model.layers.17.attention.infer_attention.paged_attention_mgr.key_cache', 'model.layers.17.attention.infer_attention.paged_attention_mgr.value_cache', 'model.layers.18.attention.infer_attention.paged_attention_mgr.key_cache', 'model.layers.18.attention.infer_attention.paged_attention_mgr.value_cache', 'model.layers.19.attention.infer_attention.paged_attention_mgr.key_cache', 'model.layers.19.attention.infer_attention.paged_attention_mgr.value_cache', 'model.layers.20.attention.infer_attention.paged_attention_mgr.key_cache', 'model.layers.20.attention.infer_attention.paged_attention_mgr.value_cache', 'model.layers.21.attention.infer_attention.paged_attention_mgr.key_cache', 'model.layers.21.attention.infer_attention.paged_attention_mgr.value_cache', 'model.layers.22.attention.infer_attention.paged_attention_mgr.key_cache', 'model.layers.22.attention.infer_attention.paged_attention_mgr.value_cache', 'model.layers.23.attention.infer_attention.paged_attention_mgr.key_cache', 'model.layers.23.attention.infer_attention.paged_attention_mgr.value_cache', 'model.layers.24.attention.infer_attention.paged_attention_mgr.key_cache', 'model.layers.24.attention.infer_attention.paged_attention_mgr.value_cache', 'model.layers.25.attention.infer_attention.paged_attention_mgr.key_cache', 'model.layers.25.attention.infer_attention.paged_attention_mgr.value_cache', 'model.layers.26.attention.infer_attention.paged_attention_mgr.key_cache', 'model.layers.26.attention.infer_attention.paged_attention_mgr.value_cache', 'model.layers.27.attention.infer_attention.paged_attention_mgr.key_cache', 'model.layers.27.attention.infer_attention.paged_attention_mgr.value_cache', 'model.layers.28.attention.infer_attention.paged_attention_mgr.key_cache', 'model.layers.28.attention.infer_attention.paged_attention_mgr.value_cache', 'model.layers.29.attention.infer_attention.paged_attention_mgr.key_cache', 'model.layers.29.attention.infer_attention.paged_attention_mgr.value_cache', 'model.layers.30.attention.infer_attention.paged_attention_mgr.key_cache', 'model.layers.30.attention.infer_attention.paged_attention_mgr.value_cache', 'model.layers.31.attention.infer_attention.paged_attention_mgr.key_cache', 'model.layers.31.attention.infer_attention.paged_attention_mgr.value_cache'] are not loaded.
2024-11-01 00:13:51,093 - mindformers[mindformers/models/modeling_utils.py:1436] - INFO - weights in /home/ma-user/work/checkpoint_download/llama2/llama2_7b.ckpt are loaded
2024-11-01 00:13:51,094 - mindformers[mindformers/models/modeling_utils.py:591] - INFO - Set jit config for jit level:O0 and infer boost:on.
2024-11-01 00:13:51,094 - mindformers[mindformers/models/auto/auto_factory.py:365] - INFO - model built successfully!
agent : 0.000000 started
all agents started
-